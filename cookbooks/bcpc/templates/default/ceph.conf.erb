################################################
#
#              Generated by Chef
#
################################################

[global]
    fsid = <%=get_config('ceph-fs-uuid')%>
    auth cluster required = cephx
    auth service required = cephx
    auth client required = cephx
    public network = <%= @node["bcpc"]["storage"]["cidr"] %>
    cluster network = <%= @node["bcpc"]["storage"]["cidr"] %>
    keyring = /etc/ceph/$cluster.$name.keyring
    mon host = <%= @servers.map{|s| s["bcpc"]["storage"]["ip"] + ":6789"}.join(', ') %>
    mon_pg_warn_max_per_osd = <%= @node['bcpc']['ceph']['max_pgs_per_osd'] %>
    mon pg warn max object skew = <%= @node['bcpc']['ceph']['pg_warn_max_obj_skew'] %>
<% if @node["bcpc"]["ceph"]["rebalance"] %>
    paxos_propose_interval = 60
<% end %>

[mon]
    keyring = /var/lib/ceph/mon/$cluster-$id/keyring
    debug paxos = 0/5

# Not using CephFS but mds remains until it's removed at some later point.
#[mds]
#    keyring = /var/lib/ceph/mds/$cluster-$id/keyring

[osd]
    keyring = /var/lib/ceph/osd/$cluster-$id/keyring
    osd journal size = <%=node['bcpc']['ceph']['journal_size']%>
    filestore xattr use omap = true
    osd mkfs type = xfs
    osd mkfs options xfs = -f -i size=2048
    osd mount options xfs = noexec,nodev,noatime,nodiratime,barrier=0,discard
    osd crush update on start = false
    osd client op priority = 63
    osd scrub load threshold = <%= @node['bcpc']['ceph']['osd_scrub_load_threshold'] %>
<% if @node["bcpc"]["ceph"]["rebalance"] %>
    osd recovery max active = 1
    osd max backfills  = 1
    osd op threads = 10
    osd recovery op priority = 1
    osd mon report interval min = 30
<% end %>

[client.admin]
    # dont want the admin client creating sockets/logs
    admin socket =
    log file =

[client.cinder]
    rbd cache = true
    rbd cache writethrough until flush = true
    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok # must be writable by QEMU and allowed by SELinux or AppArmor
    log file = /var/log/qemu/qemu-guest-$pid.log
    rbd concurrent management ops = 20

[client.glance]
    rbd cache = true
    rbd cache writethrough until flush = true
    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok # must be writable by QEMU and allowed by SELinux or AppArmor
    log file = /var/log/qemu/qemu-guest-$pid.log
    rbd concurrent management ops = 20

[client.radosgw.gateway]
    host = <%=node['hostname']%>
    keyring = /var/lib/ceph/radosgw/$cluster-$id/keyring
    log file = /var/log/radosgw/radosgw.log
    rgw data = /var/lib/ceph/radosgw/$cluser-$id
    rgw enable ops log = false
    rgw enable usage log = false
    rgw print continue = false
    rgw socket path = /var/lib/ceph/radosgw/rgw.sock
    rgw frontends = civetweb port=<%= node['bcpc']['ports']['radosgw'] %>
    # Increased to 1 to log HTTP return codes
    # http://tracker.ceph.com/issues/12432
    debug rgw = 1/0

<% if config_defined('keystone-admin-token') %>
    rgw keystone url = openstack.<%=node['bcpc']['cluster_domain']%>:35358
    rgw keystone admin token = <%=get_config('keystone-admin-token')%>
    rgw keystone accepted roles = Admin admin Member _member_
    rgw keystone token cache size = 1000
    rgw keystone revocation interval = 1200
    rgw s3 auth use keystone = true
<% end %>
    rgw dns name = s3.<%=node['bcpc']['cluster_domain'] %>
